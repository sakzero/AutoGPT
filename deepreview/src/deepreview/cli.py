from __future__ import annotations

import argparse
import fnmatch
import json
import os
import shutil
import subprocess
import sys
import textwrap
from dataclasses import dataclass
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, List, Optional, Sequence

import yaml

from .core import sarif as sarif_writer
from .core.analyzer import ProjectAnalyzer
from .core.audit import HeuristicAuditor
from .core.context import CodeContextManager
from .core.dataflow import analyze_taint
from .core.git_ops import get_changed_files, get_git_diff, get_project_snapshot
from .core.llm_client import LLMClient
from .core.protocols import ProtocolAdvisor
from .core.quality import collect_quality_findings
from .core.reporting import write_report
from .core.state import AutomationState, PhaseContext
from .core.style import analyze_style
from .core.tracer import RunTracer
from .config import Config

SCAN_MODE_PRESETS = {
    "quick": {"llm_max_findings": 3},
    "standard": {"llm_max_findings": 5},
    "deep": {"llm_max_findings": 10},
}

SEVERITY_LEVELS = ("critical", "high", "medium", "low", "info")
REPRO_ATTEMPT_LIMIT = 3


@dataclass
class DiffSection:
    text: str
    files: list[str]


@dataclass
class DiffChunk:
    text: str
    files: list[str]
    section_count: int
    diff_bytes: int


def _default_run_name() -> str:
    return datetime.now(timezone.utc).strftime("run-%Y%m%d-%H%M%S")


def _stage_workspace(source_path: str, tracer: RunTracer) -> str:
    workspace_dir = tracer.run_directory() / "workspace"
    if workspace_dir.exists():
        shutil.rmtree(workspace_dir, ignore_errors=True)

    ignore = shutil.ignore_patterns(
        ".git",
        ".deepreview-env",
        "deepreview_runs",
        "__pycache__",
        "*.pyc",
    )

    try:
        shutil.copytree(source_path, workspace_dir, ignore=ignore, dirs_exist_ok=True)
    except Exception as exc:  # noqa: BLE001
        tracer.log_event("workspace", "failed", {"error": str(exc)})
        return source_path

    tracer.log_event("workspace", "staged", {"path": str(workspace_dir)})
    return str(workspace_dir)


def _archive_run_directory(run_dir: Path, archive_path: Optional[str] = None) -> Path:
    base_name: Path
    if archive_path:
        target = Path(archive_path)
        target.parent.mkdir(parents=True, exist_ok=True)
        base_name = target.with_suffix("")
    else:
        base_name = run_dir
    archive_file = shutil.make_archive(str(base_name), "zip", root_dir=run_dir)
    return Path(archive_file)


def _build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(description="DeepReview: Autonomous Python Code Auditor")
    parser.add_argument("path", nargs="?", default=".", help="Path to target project")
    parser.add_argument("--config", help="Path to DeepReview YAML config (overrides CLI options for each target).")
    parser.add_argument(
        "--llm-retries",
        type=int,
        default=Config.LLM_MAX_RETRIES,
        help="Maximum LLM retry attempts per request.",
    )
    parser.add_argument("--run-name", help="Override autogenerated run name used for telemetry/artifacts.")
    parser.add_argument("--metadata-path", help="Write final run metadata JSON to the given path for CI integration.")
    parser.add_argument(
        "--cleanup-workspace",
        action="store_true",
        help="Remove staged workspace after run completes.",
    )
    parser.add_argument(
        "--fail-on-confirmed",
        action="store_true",
        help="Exit with status code 2 when findings meet the severity threshold.",
    )
    parser.add_argument(
        "--fail-on-severity",
        choices=("critical", "high", "medium", "low"),
        default="high",
        help="Minimum severity that triggers --fail-on-confirmed (default: high).",
    )
    parser.add_argument(
        "--archive-run",
        nargs="?",
        const="",
        help="Zip the run directory and optionally write to a custom path.",
    )
    parser.add_argument(
        "--scan-mode",
        choices=tuple(SCAN_MODE_PRESETS.keys()),
        default="standard",
        help="Select scan depth: quick, standard, or deep.",
    )
    parser.add_argument(
        "--sarif-path",
        help="Write SARIF 2.1.0 output to this path (defaults to report path with .sarif).",
    )
    parser.add_argument(
        "--summary-path",
        help="Write run summary (severity distribution and top findings) to this path (Markdown).",
    )
    parser.add_argument(
        "--suppress-pattern",
        action="append",
        help="Substring or glob pattern to suppress findings (matches title/file/function/description).",
    )
    parser.add_argument(
        "--suppress-pattern-file",
        help="Path to newline-delimited patterns for suppressing findings.",
    )
    parser.add_argument(
        "--changed-files",
        nargs="*",
        help="Limit diff and context to these relative file paths.",
    )
    parser.add_argument(
        "--changed-files-list",
        help="Path to a file containing newline-separated paths to scan.",
    )
    parser.add_argument(
        "--diff-target",
        help="Git reference to compare against when computing changed files (e.g., origin/main).",
    )
    parser.add_argument(
        "--init-config",
        help="Create a starter deepreview.yml at the given path and exit without running a scan.",
    )
    return parser


def _load_config(path: str) -> dict[str, Any]:
    config_path = Path(path)
    if not config_path.exists():
        raise FileNotFoundError(f"Config file not found: {path}")
    data = yaml.safe_load(config_path.read_text(encoding="utf-8")) or {}
    if not isinstance(data, dict):
        raise ValueError("Config file must define a mapping at the top level.")
    return data


def _merge_options(
    base: argparse.Namespace,
    defaults: dict[str, Any],
    target_cfg: dict[str, Any],
) -> argparse.Namespace:
    merged: dict[str, Any] = vars(base).copy()
    merged.pop("config", None)
    merged.update(defaults or {})
    merged.update(target_cfg or {})
    if not merged.get("path"):
        raise ValueError("Each target in the config must define a 'path'.")
    return argparse.Namespace(**merged)


def _collect_changed_paths(opts: argparse.Namespace, repo_path: str) -> list[str]:
    paths: list[str] = []
    if getattr(opts, "changed_files", None):
        paths.extend(opts.changed_files)
    list_path = getattr(opts, "changed_files_list", None)
    if list_path:
        file_path = Path(list_path)
        if file_path.exists():
            entries = [line.strip() for line in file_path.read_text(encoding="utf-8").splitlines()]
            paths.extend(entries)
    if not paths and getattr(opts, "diff_target", None):
        paths.extend(get_changed_files(repo_path, opts.diff_target))
    normalized = sorted({p.replace("\\", "/").strip() for p in paths if p.strip()})
    if normalized:
        print(f"[Git] Incremental scan limited to {len(normalized)} file(s).")
    return normalized


def _is_section_boundary(line: str) -> bool:
    stripped = line.strip()
    if not stripped:
        return False
    return (
        stripped.startswith("diff --git ")
        or stripped.startswith("--- Untracked File:")
        or stripped.startswith("--- File:")
    )


def _extract_paths_from_line(line: str) -> list[str]:
    stripped = line.strip()
    candidates: list[str] = []
    if stripped.startswith("+++ b/"):
        candidates.append(stripped[6:])
    elif stripped.startswith("diff --git "):
        parts = stripped.split()
        if len(parts) >= 4 and parts[3].startswith("b/"):
            candidates.append(parts[3][2:])
    elif stripped.startswith("--- File:"):
        candidates.append(stripped.split(":", 1)[1].strip())
    elif stripped.startswith("--- Untracked File:"):
        candidates.append(stripped.split(":", 1)[1].strip())
    cleaned = []
    for candidate in candidates:
        entry = candidate.strip().replace("\\", "/")
        if entry and entry != "/dev/null":
            cleaned.append(entry)
    return cleaned


def _split_diff_sections(diff_text: str) -> list[DiffSection]:
    if not diff_text or not diff_text.strip():
        return []
    sections: list[DiffSection] = []
    preamble: list[str] = []
    buffer: list[str] = []
    files: set[str] = set()
    started = False

    def _flush() -> None:
        nonlocal buffer, files
        if buffer:
            text = "\n".join(buffer).strip()
            if text:
                sections.append(DiffSection(text=text, files=sorted(files)))
        buffer = []
        files = set()

    for raw_line in diff_text.splitlines():
        boundary = _is_section_boundary(raw_line)
        if not started:
            if boundary:
                buffer = preamble + [raw_line]
                files = set(_extract_paths_from_line(raw_line))
                preamble = []
                started = True
            else:
                preamble.append(raw_line)
            continue

        if boundary:
            _flush()
            buffer = [raw_line]
            files = set(_extract_paths_from_line(raw_line))
            continue

        buffer.append(raw_line)
        for entry in _extract_paths_from_line(raw_line):
            files.add(entry)

    if started:
        _flush()
    elif preamble:
        text = "\n".join(preamble).strip()
        if text:
            sections.append(DiffSection(text=text, files=[]))
    return sections


def _chunk_diff_sections(
    sections: list[DiffSection],
    max_chars: int,
    max_sections: int,
) -> list[DiffChunk]:
    if not sections:
        return []
    chunks: list[DiffChunk] = []
    buffer: list[str] = []
    files: set[str] = set()
    char_budget = max_chars if max_chars > 0 else None
    section_limit = max_sections if max_sections > 0 else None
    accumulated_chars = 0
    accumulated_sections = 0

    def _flush() -> None:
        nonlocal buffer, files, accumulated_chars, accumulated_sections
        if not buffer:
            return
        text = "\n\n".join(buffer).strip()
        if text:
            chunks.append(
                DiffChunk(
                    text=text,
                    files=sorted(files),
                    section_count=accumulated_sections,
                    diff_bytes=len(text),
                )
            )
        buffer = []
        files = set()
        accumulated_chars = 0
        accumulated_sections = 0

    for section in sections:
        if not section.text.strip():
            continue
        section_len = len(section.text) + 2
        should_flush = False
        if buffer:
            if char_budget and (accumulated_chars + section_len) > char_budget:
                should_flush = True
            if section_limit and accumulated_sections >= section_limit:
                should_flush = True
        if should_flush:
            _flush()
        buffer.append(section.text)
        accumulated_chars += section_len
        accumulated_sections += 1
        files.update(section.files)

    _flush()
    return chunks


def _prepare_llm_chunks(
    diff_text: str,
    changed_files: list[str],
    analysis_source: str,
) -> tuple[list[DiffChunk], int, bool]:
    sections = _split_diff_sections(diff_text)
    if not sections and diff_text and diff_text.strip():
        sections = [DiffSection(text=diff_text.strip(), files=sorted(set(changed_files)))]
    chunks = _chunk_diff_sections(sections, Config.LLM_DIFF_CHUNK_CHARS, Config.LLM_DIFF_MAX_SECTIONS)
    original_count = len(chunks)
    truncated = False

    max_chunks = Config.LLM_MAX_CHUNKS if Config.LLM_MAX_CHUNKS > 0 else None
    if analysis_source == "snapshot":
        snapshot_limit = Config.LLM_MAX_SNAPSHOT_CHUNKS if Config.LLM_MAX_SNAPSHOT_CHUNKS > 0 else None
        if snapshot_limit is not None:
            max_chunks = snapshot_limit if max_chunks is None else min(max_chunks, snapshot_limit)

    if max_chunks is not None and original_count > max_chunks:
        print(f"[LLM] Diff spans {original_count} chunk(s); trimming to first {max_chunks}.")
        chunks = chunks[:max_chunks]
        truncated = True

    return chunks, original_count, truncated


def _perform_llm_review(
    llm_client: LLMClient,
    diff_chunks: list[DiffChunk],
    available_chunk_count: int,
    chunks_truncated: bool,
    full_diff_text: str,
    full_context_text: str,
    context_manager: CodeContextManager,
    project_metadata: dict[str, Any],
    changed_files: list[str],
    analysis_source: str,
    protocol_hints: Optional[str],
    scan_mode: str,
    max_findings: Optional[int],
) -> dict[str, Any]:
    metadata = _prepare_llm_metadata(project_metadata, changed_files, analysis_source, scan_mode)
    enabled = getattr(llm_client, "enabled", True)
    if not enabled:
        response = llm_client.review_changes(
            diff_content=full_diff_text,
            context_content=full_context_text,
            metadata=metadata,
            protocol_hints=protocol_hints,
            max_findings=max_findings,
        )
        response.setdefault("chunks", [])
        response.setdefault("chunks", [])
        response.setdefault(
            "chunk_overview",
            {
                "processed": len(diff_chunks),
                "available": available_chunk_count,
                "truncated": chunks_truncated,
            },
        )
        return response

    if not diff_chunks:
        return {
            "summary": "未检测到 Python 变更，LLM 审计已跳过。",
            "insights": [],
            "findings": [],
            "chunks": [],
            "chunk_overview": {
                "processed": 0,
                "available": 0,
                "truncated": False,
            },
        }

    combined_findings: list[dict[str, Any]] = []
    combined_insights: list[str] = []
    chunk_summaries: list[tuple[int, str]] = []
    chunk_stats: list[dict[str, Any]] = []
    total_chunks = len(diff_chunks)

    for idx, chunk in enumerate(diff_chunks, start=1):
        chunk_metadata = dict(metadata)
        chunk_metadata.update(
            {
                "chunk_index": idx,
                "chunk_total": total_chunks,
                "chunk_available": available_chunk_count,
                "chunk_section_count": chunk.section_count,
                "chunk_file_count": len(chunk.files),
                "chunk_diff_bytes": chunk.diff_bytes,
                "chunks_truncated": chunks_truncated,
            }
        )
        chunk_context = context_manager.retrieve_context(chunk.text, include_paths=chunk.files or None)
        response = llm_client.review_changes(
            diff_content=chunk.text,
            context_content=chunk_context,
            metadata=chunk_metadata,
            protocol_hints=protocol_hints,
            max_findings=max_findings,
        )
        chunk_stats.append(
            {
                "index": idx,
                "file_count": len(chunk.files),
                "section_count": chunk.section_count,
                "diff_bytes": chunk.diff_bytes,
                "raw_summary": response.get("summary") or "",
            }
        )
        combined_findings.extend(response.get("findings") or [])
        combined_insights.extend(response.get("insights") or [])
        summary = (response.get("summary") or "").strip()
        if summary:
            chunk_summaries.append((idx, summary))

    if total_chunks == 1 and chunk_summaries:
        aggregated_summary = chunk_summaries[0][1]
    else:
        aggregated_summary = " ".join(f"[片段 {idx}] {text}" for idx, text in chunk_summaries).strip()
    return {
        "summary": aggregated_summary,
        "insights": combined_insights,
        "findings": combined_findings,
        "chunks": chunk_stats,
        "chunk_overview": {
            "processed": total_chunks,
            "available": available_chunk_count,
            "truncated": chunks_truncated,
        },
    }


def _discover_python_targets(root_path: Path, limit: int = 5) -> list[str]:
    targets: list[str] = []
    for child in sorted(root_path.iterdir(), key=lambda p: p.name.lower()):
        if not child.is_dir():
            continue
        if child.name.startswith(".") or child.name in {"deepreview_runs", "__pycache__"}:
            continue
        if _contains_python(child):
            rel = child.relative_to(root_path).as_posix()
            targets.append(rel)
        if len(targets) >= limit:
            break
    return targets


def _contains_python(directory: Path, depth: int = 2) -> bool:
    if depth < 0:
        return False
    try:
        for item in directory.iterdir():
            if item.name.startswith("."):
                continue
            if item.is_file() and item.suffix == ".py":
                return True
            if item.is_dir() and _contains_python(item, depth - 1):
                return True
    except OSError:
        return False
    return False


def _generate_config_template(target_path: str, output_path: str, default_scan_mode: str) -> Path:
    root = Path(target_path).resolve()
    output = Path(output_path)
    if output.exists():
        raise FileExistsError(f"Config file already exists: {output}")

    defaults = {
        "scan_mode": default_scan_mode,
        "diff_target": "origin/main",
        "metadata_path": "artifacts/deepreview-meta.json",
        "summary_path": "artifacts/deepreview-summary.md",
        "suppress_patterns": ["third_party/*"],
    }
    targets = [
        {"path": ".", "run_name": "audit-root"}
    ]
    for subpath in _discover_python_targets(root):
        run_name = subpath.replace("/", "-") or "root"
        targets.append({"path": subpath, "run_name": f"audit-{run_name}"})

    template = {"defaults": defaults, "targets": targets}
    output.parent.mkdir(parents=True, exist_ok=True)
    output.write_text(yaml.safe_dump(template, sort_keys=False, allow_unicode=True), encoding="utf-8")
    return output



def _format_severity_summary(summary: dict[str, int]) -> str:
    parts = []
    for level in SEVERITY_LEVELS:
        parts.append(f"{level}:{summary.get(level, 0)}")
    return " | ".join(parts)


def _severity_rank(value: str) -> int:
    normalized = (value or "").lower()
    try:
        return SEVERITY_LEVELS.index(normalized)
    except ValueError:
        return len(SEVERITY_LEVELS)


def _collect_top_findings(
    llm_findings: list[dict[str, Any]],
    audit_findings: list[dict[str, Any]],
    style_findings: list[dict[str, Any]],
    taint_findings: list[dict[str, Any]],
) -> list[tuple[str, dict[str, Any]]]:
    combined: list[tuple[str, dict[str, Any]]] = []
    for label, findings in (
        ("LLM", llm_findings),
        ("Heuristic", audit_findings),
        ("Style", style_findings),
        ("Taint", taint_findings),
    ):
        for finding in findings or []:
            combined.append((label, finding))
    combined.sort(key=lambda item: _severity_rank(str(item[1].get("severity", ""))))
    return combined[:3]


def _render_run_summary(
    severity_summary: dict[str, int],
    llm_findings: list[dict[str, Any]],
    audit_findings: list[dict[str, Any]],
    style_findings: list[dict[str, Any]],
    taint_findings: list[dict[str, Any]],
) -> str:
    lines = []
    lines.append("")
    lines.append("=== DeepReview Summary ===")
    lines.append(f"Severity distribution: {_format_severity_summary(severity_summary)}")
    top_findings = _collect_top_findings(llm_findings, audit_findings, style_findings, taint_findings)
    if not top_findings:
        lines.append("No actionable findings detected.")
        return "\n".join(lines)
    lines.append("Top findings:")
    for idx, (source, finding) in enumerate(top_findings, start=1):
        title = finding.get("title") or "Untitled"
        severity = finding.get("severity") or "info"
        file_ref = finding.get("file") or "n/a"
        line_ref = finding.get("line")
        line_part = f":{line_ref}" if line_ref else ""
        lines.append(f"  {idx}. [{severity}] ({source}) {title} - {file_ref}{line_part}")
    return "\n".join(lines)


def _normalize_pattern_list(raw: Any) -> list[str]:
    if not raw:
        return []
    if isinstance(raw, str):
        values = [raw]
    elif isinstance(raw, Sequence):
        values = list(raw)
    else:
        return []
    patterns: list[str] = []
    for value in values:
        if isinstance(value, str) and value.strip():
            patterns.append(value.strip())
    return patterns


def _load_suppress_patterns(opts: argparse.Namespace, config_patterns: Optional[list[str]] = None) -> list[str]:
    patterns: list[str] = []
    if getattr(opts, "suppress_pattern", None):
        patterns.extend(str(p).strip() for p in opts.suppress_pattern if str(p).strip())
    pattern_file = getattr(opts, "suppress_pattern_file", None)
    if pattern_file:
        file_path = Path(pattern_file)
        if file_path.exists():
            for line in file_path.read_text(encoding="utf-8").splitlines():
                line = line.strip()
                if line:
                    patterns.append(line)
    if config_patterns:
        for pattern in config_patterns:
            if pattern and pattern not in patterns:
                patterns.append(pattern)
    return patterns


def _matches_pattern(text_snippets: list[str], pattern: str) -> bool:
    for snippet in text_snippets:
        if not snippet:
            continue
        snippet_lower = snippet.lower()
        pattern_lower = pattern.lower()
        if pattern_lower in snippet_lower:
            return True
        if fnmatch.fnmatch(snippet_lower, pattern_lower):
            return True
    return False


def _filter_findings(findings: list[dict[str, Any]], patterns: list[str]) -> list[dict[str, Any]]:
    if not patterns or not findings:
        return findings
    filtered: list[dict[str, Any]] = []
    for finding in findings:
        haystack = [
            str(finding.get("title", "")),
            str(finding.get("description", "")),
            str(finding.get("file", "")),
            str(finding.get("function", "")),
            str(finding.get("recommendation", "")),
        ]
        if any(_matches_pattern(haystack, pattern) for pattern in patterns):
            continue
        filtered.append(finding)
    return filtered


def _module_name_from_file(relative_path: str) -> str:
    module_path = Path(relative_path).with_suffix("")
    return module_path.as_posix().replace("/", ".").replace("\\", ".")


def _normalize_function_path(function_name: str, module_name: str) -> Optional[List[str]]:
    if not function_name:
        return None
    clean = function_name
    if module_name and clean.startswith(f"{module_name}."):
        clean = clean[len(module_name) + 1 :]
    clean = clean.strip(".")
    if not clean:
        return None
    parts = [part for part in clean.split(".") if part]
    return parts or None


def _apply_suppressions(findings: list[dict[str, Any]], patterns: list[str]) -> tuple[list[dict[str, Any]], int]:
    filtered = _filter_findings(findings or [], patterns)
    removed = len(findings or []) - len(filtered)
    return filtered, removed


def _compute_severity_summary(collections: Sequence[Sequence[dict[str, Any]]]) -> dict[str, int]:
    summary = {level: 0 for level in SEVERITY_LEVELS}
    for group in collections:
        for finding in group or []:
            level = str(finding.get("severity", "info")).lower()
            if level not in summary:
                level = "info"
            summary[level] += 1
    return summary


def _should_fail(summary: dict[str, int], threshold: str) -> bool:
    threshold_rank = _severity_rank(threshold)
    for level, count in summary.items():
        if not count:
            continue
        if _severity_rank(level) <= threshold_rank:
            return True
    return False


def _prepare_llm_metadata(
    project_metadata: dict[str, Any],
    changed_files: list[str],
    analysis_source: str,
    scan_mode: str,
) -> dict[str, Any]:
    return {
        "project_metadata": project_metadata,
        "changed_files": changed_files,
        "analysis_source": analysis_source,
        "scan_mode": scan_mode,
    }


def _write_summary_artifacts(summary_text: str, summary_path: Optional[str]) -> dict[str, str]:
    artifacts: dict[str, str] = {}
    if summary_path:
        summary_file = Path(summary_path)
        summary_file.parent.mkdir(parents=True, exist_ok=True)
        summary_file.write_text(summary_text + "\n", encoding="utf-8")
        artifacts["summary"] = str(summary_file)
    github_summary = os.environ.get("GITHUB_STEP_SUMMARY")
    if github_summary:
        gh_path = Path(github_summary)
        gh_path.parent.mkdir(parents=True, exist_ok=True)
        with gh_path.open("a", encoding="utf-8") as handle:
            handle.write(summary_text + "\n")
        artifacts["github_summary"] = str(gh_path)
    if summary_text.strip():
        print(summary_text)
    return artifacts


def _write_metadata_file(
    metadata_path: Optional[str],
    status: str,
    run_directory: str,
    severity_summary: dict[str, int],
    counts: dict[str, int],
    reproduction_count: int,
) -> Optional[Path]:
    if not metadata_path:
        return None
    payload = {
        "status": status,
        "run_directory": run_directory,
        "details": {
            "severity_summary": severity_summary,
            "llm_findings": counts.get("llm", 0),
            "heuristic_findings": counts.get("audit", 0),
            "style_findings": counts.get("style", 0),
            "taint_findings": counts.get("taint", 0),
            "quality_findings": counts.get("quality", 0),
            "reproduction_attempts": reproduction_count,
        },
    }
    output = Path(metadata_path)
    output.parent.mkdir(parents=True, exist_ok=True)
    output.write_text(json.dumps(payload, indent=2), encoding="utf-8")
    return output


def _safe_int(value: Any) -> Optional[int]:
    try:
        return int(value)
    except (TypeError, ValueError):
        return None


def _select_reproduction_candidates(findings: list[dict[str, Any]]) -> list[dict[str, Any]]:
    if not findings:
        return []
    sorted_findings = sorted(
        findings,
        key=lambda item: _severity_rank(str(item.get("severity", "info")).lower()),
    )
    return sorted_findings[:REPRO_ATTEMPT_LIMIT]


def _write_repro_script(
    repro_dir: Path,
    workspace_path: str,
    finding: dict[str, Any],
    index: int,
) -> Path:
    repro_dir.mkdir(parents=True, exist_ok=True)
    script_path = repro_dir / f"attempt_{index}.py"
    target_file = finding.get("file") or ""
    line_value = _safe_int(finding.get("line"))
    snippet_radius = 5
    script_content = textwrap.dedent(
        f"""
        import json
        from pathlib import Path

        WORKSPACE = Path(r\"{workspace_path}\").resolve()
        TARGET_FILE = WORKSPACE / r\"{target_file}\"
        TARGET_LINE = {line_value if line_value is not None else 'None'}

        def load_snippet(path: Path, line_no):
            if not path.exists():
                raise FileNotFoundError(f\"{{path}} not found\")
            data = path.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines()
            if line_no is None:
                start, end = 0, min(len(data), 20)
            else:
                start = max(line_no - {snippet_radius} - 1, 0)
                end = min(line_no + {snippet_radius}, len(data))
            return \"\\n\".join(data[start:end])

        if __name__ == \"__main__\":
            snippet = load_snippet(TARGET_FILE, TARGET_LINE)
            output = {{
                \"file\": str(TARGET_FILE),
                \"line\": TARGET_LINE,
                \"snippet\": snippet,
            }}
            print(\"=== DeepReview reproduction context ===\")
            print(json.dumps(output, indent=2, ensure_ascii=False))
        """
    ).strip()
    script_path.write_text(script_content, encoding="utf-8")
    return script_path


def _execute_repro_script(script_path: Path, workspace_path: str) -> tuple[bool, str, str]:
    env = os.environ.copy()
    env.setdefault("PYTHONUTF8", "1")
    python_path = env.get("PYTHONPATH")
    env["PYTHONPATH"] = os.pathsep.join(filter(None, [workspace_path, python_path]))
    try:
        proc = subprocess.run(
            [sys.executable, str(script_path)],
            cwd=workspace_path,
            capture_output=True,
            text=True,
            timeout=60,
            env=env,
        )
        return proc.returncode == 0, proc.stdout, proc.stderr
    except subprocess.TimeoutExpired as exc:
        return False, exc.stdout or "", (exc.stderr or "") + "\nTimeout expired"
    except Exception as exc:  # noqa: BLE001
        return False, "", str(exc)


def _attempt_reproduction(
    taint_findings: list[dict[str, Any]],
    tracer: RunTracer,
    workspace_path: str,
) -> tuple[list[dict[str, Any]], Optional[Path]]:
    candidates = _select_reproduction_candidates(taint_findings)
    if not candidates:
        return [], None
    repro_dir = tracer.run_directory() / "repro_attempts"
    attempts: list[dict[str, Any]] = []
    for idx, finding in enumerate(candidates, start=1):
        script_path = _write_repro_script(repro_dir, workspace_path, finding, idx)
        success, stdout, stderr = _execute_repro_script(script_path, workspace_path)
        tracer.log_attempt(idx, stdout, stderr, success)
        attempts.append(
            {
                "title": finding.get("title"),
                "severity": finding.get("severity"),
                "file": finding.get("file"),
                "line": finding.get("line"),
                "script": str(script_path),
                "success": success,
                "stdout": stdout[-2000:],
                "stderr": stderr[-2000:],
            }
        )
    return attempts, repro_dir


def _run_single_target(opts: argparse.Namespace, config_patterns: Optional[list[str]] = None) -> int:
    suppress_patterns = _load_suppress_patterns(opts, config_patterns=config_patterns)
    original_path = os.path.abspath(opts.path)
    run_name = opts.run_name or _default_run_name()
    tracer = RunTracer(run_name)
    state = AutomationState(run_name=run_name, max_iterations=Config.AUTOMATION_MAX_ITERATIONS)
    tracer.persist_state(state.snapshot())
    workspace_path = _stage_workspace(original_path, tracer)
    tracer.set_target(original_path, "pending", workspace_path)
    report_path = Path.cwd() / "deepreview_report.json"
    sarif_path = Path(opts.sarif_path) if opts.sarif_path else report_path.with_suffix(".sarif")
    artifacts: dict[str, Any] = {"run_directory": str(tracer.run_directory())}
    report_status = "completed"
    exit_code = 0

    try:
        changed_files = _collect_changed_paths(opts, original_path)
        analyzer = ProjectAnalyzer(workspace_path)
        with PhaseContext(state, "analyzer"):
            analyzer.detect_entry_point()
            project_metadata = analyzer.gather_metadata()

        diff_text = ""
        analysis_source = "diff"
        diff_target = getattr(opts, "diff_target", None)
        with PhaseContext(state, "diff"):
            diff_text = get_git_diff(original_path, changed_files, diff_target)
            if not diff_text:
                analysis_source = "snapshot"
                diff_text = get_project_snapshot(workspace_path, include_paths=changed_files or None)

        context_manager = CodeContextManager(workspace_path)
        with PhaseContext(state, "context"):
            context_manager.build_index()
            context_text = context_manager.retrieve_context(diff_text, include_paths=changed_files or None)

        advisor = ProtocolAdvisor()
        protocol_evidence = advisor.gather(diff_text, context_text)
        protocol_hints = advisor.describe(diff_text, context_text)

        preset = SCAN_MODE_PRESETS.get(opts.scan_mode, SCAN_MODE_PRESETS["standard"])
        llm_client = LLMClient(max_retries=opts.llm_retries)
        diff_chunks, chunk_available_count, chunk_truncated = _prepare_llm_chunks(
            diff_text, changed_files, analysis_source
        )
        with PhaseContext(state, "llm_review"):
            llm_response = _perform_llm_review(
                llm_client=llm_client,
                diff_chunks=diff_chunks,
                available_chunk_count=chunk_available_count,
                chunks_truncated=chunk_truncated,
                full_diff_text=diff_text,
                full_context_text=context_text,
                context_manager=context_manager,
                project_metadata=project_metadata,
                changed_files=changed_files,
                analysis_source=analysis_source,
                protocol_hints=protocol_hints,
                scan_mode=opts.scan_mode,
                max_findings=preset.get("llm_max_findings"),
            )

        auditor = HeuristicAuditor(scan_context=Config.HEURISTIC_SCAN_CONTEXT)
        audit_findings = auditor.run(diff_text, context_text, analysis_source=analysis_source)
        style_findings = analyze_style(workspace_path, include_paths=changed_files or None)
        taint_findings = analyze_taint(workspace_path, include_paths=changed_files or None)
        quality_findings = collect_quality_findings(workspace_path)

        llm_findings, removed_llm = _apply_suppressions(llm_response.get("findings") or [], suppress_patterns)
        audit_findings, removed_audit = _apply_suppressions(audit_findings, suppress_patterns)
        style_findings, removed_style = _apply_suppressions(style_findings, suppress_patterns)
        taint_findings, removed_taint = _apply_suppressions(taint_findings, suppress_patterns)
        quality_findings, removed_quality = _apply_suppressions(quality_findings, suppress_patterns)

        severity_summary = _compute_severity_summary(
            [llm_findings, audit_findings, style_findings, taint_findings, quality_findings]
        )
        reproduction_attempts, repro_dir = _attempt_reproduction(taint_findings, tracer, workspace_path)
        if repro_dir:
            artifacts["reproduction_dir"] = str(repro_dir)

        summary_text = _render_run_summary(
            severity_summary,
            llm_findings,
            audit_findings,
            style_findings,
            taint_findings,
        )
        summary_artifacts = _write_summary_artifacts(summary_text, opts.summary_path)
        artifacts.update(summary_artifacts)

        llm_chunks = llm_response.get("chunks") or []
        chunk_overview = llm_response.get("chunk_overview") or {
            "processed": len(llm_chunks),
            "available": len(llm_chunks),
            "truncated": False,
        }
        counts = {
            "llm": len(llm_findings),
            "audit": len(audit_findings),
            "style": len(style_findings),
            "taint": len(taint_findings),
            "quality": len(quality_findings),
        }
        metadata = {
            "project_metadata": project_metadata,
            "protocol_evidence": [
                {"name": evidence.name, "files": sorted(evidence.files), "details": evidence.details[:5]}
                for evidence in protocol_evidence
            ],
            "changed_files": changed_files,
            "context_stats": {
                "diff_bytes": len(diff_text or ""),
                "context_bytes": len(context_text or ""),
                "changed_file_count": len(changed_files),
                "chunk_count": chunk_overview.get("processed", len(llm_chunks)),
            },
            "suppression": {
                "patterns": suppress_patterns,
                "llm": removed_llm,
                "heuristic": removed_audit,
                "style": removed_style,
                "taint": removed_taint,
                "quality": removed_quality,
                "total": removed_llm + removed_audit + removed_style + removed_taint + removed_quality,
            },
            "run_name": run_name,
            "analysis_source": analysis_source,
            "scan_mode": opts.scan_mode,
            "llm_max_findings": preset.get("llm_max_findings"),
            "reproduction_count": len(reproduction_attempts),
            "diff_target": getattr(opts, "diff_target", None),
            "quality_tools": ["ruff", "bandit"],
            "llm_chunking": {
                "processed_chunks": chunk_overview.get("processed", len(llm_chunks)),
                "available_chunks": chunk_overview.get("available", len(llm_chunks)),
                "chunks_truncated": chunk_overview.get("truncated", False),
                "chunk_char_limit": Config.LLM_DIFF_CHUNK_CHARS,
                "chunk_section_limit": Config.LLM_DIFF_MAX_SECTIONS,
                "max_chunks": Config.LLM_MAX_CHUNKS,
                "max_snapshot_chunks": Config.LLM_MAX_SNAPSHOT_CHUNKS,
            },
            "llm_chunks": llm_chunks,
        }
        analysis = {
            "source": analysis_source,
            "scan_mode": opts.scan_mode,
            "summary": llm_response.get("summary") or "",
            "insights": llm_response.get("insights") or [],
            "llm_findings": llm_findings,
            "audit_findings": audit_findings,
            "style_findings": style_findings,
            "taint_findings": taint_findings,
            "quality_findings": quality_findings,
            "protocol_hints": protocol_hints,
            "project": {"path": original_path, "workspace": workspace_path},
            "metadata": metadata,
            "severity_summary": severity_summary,
        }
        report_data = {
            "status": report_status,
            "target": {"original": original_path, "workspace": workspace_path},
            "analysis": analysis,
            "reproduction": reproduction_attempts,
            "artifacts": artifacts,
        }

        write_report(str(report_path), report_data)
        sarif_writer.write_sarif(report_data, str(sarif_path))
        artifacts["report"] = str(report_path)
        artifacts["sarif"] = str(sarif_path)
        tracer.record_artifact("report", report_path)
        tracer.record_artifact("sarif", Path(sarif_path))
        if summary_artifacts.get("summary"):
            tracer.record_artifact("summary", Path(summary_artifacts["summary"]))
        metadata_file = _write_metadata_file(
            opts.metadata_path,
            report_status,
            artifacts["run_directory"],
            severity_summary,
            counts,
            len(reproduction_attempts),
        )
        if metadata_file:
            artifacts["metadata"] = str(metadata_file)
            tracer.record_artifact("metadata", metadata_file)
        if opts.fail_on_confirmed and _should_fail(severity_summary, opts.fail_on_severity):
            exit_code = 2

        if opts.archive_run is not None:
            archive_path = _archive_run_directory(tracer.run_directory(), opts.archive_run or None)
            artifacts["archive"] = str(archive_path)
            tracer.record_artifact("archive", archive_path)

        report_data["artifacts"] = artifacts
        write_report(str(report_path), report_data)
        sarif_writer.write_sarif(report_data, str(sarif_path))

    except Exception as exc:  # noqa: BLE001
        report_status = "failed"
        state.add_error(str(exc))
        tracer.log_event("run", "failed", {"error": str(exc)})
        exit_code = 1
    finally:
        if opts.cleanup_workspace and workspace_path != original_path and Path(workspace_path).exists():
            shutil.rmtree(workspace_path, ignore_errors=True)
        state.set_completed()
        tracer.persist_state(state.snapshot())
        tracer.finalize(report_status, report_path if report_path.exists() else None)

    return exit_code


def run_scan(opts: argparse.Namespace) -> int:
    config_patterns = getattr(opts, "_config_suppress_patterns", None)
    return _run_single_target(opts, config_patterns=config_patterns)


def main() -> None:
    parser = _build_parser()
    args = parser.parse_args()
    if args.init_config:
        try:
            template_path = _generate_config_template(args.path, args.init_config, args.scan_mode)
        except FileExistsError as exc:
            raise SystemExit(str(exc))
        print(f"[Config] Starter template written to {template_path}")
        raise SystemExit(0)
    if args.config:
        config = _load_config(args.config)
        defaults = config.get("defaults") or {}
        targets = config.get("targets") or []
        if not isinstance(targets, list) or not targets:
            raise SystemExit("Config file must define at least one target.")
        exit_codes: list[int] = []
        default_patterns = _normalize_pattern_list(defaults.get("suppress_patterns"))
        for target_cfg in targets:
            if not isinstance(target_cfg, dict):
                raise SystemExit("Each target entry must be a mapping.")
            merged = _merge_options(args, defaults, target_cfg)
            patterns = default_patterns + _normalize_pattern_list(target_cfg.get("suppress_patterns"))
            setattr(merged, "_config_suppress_patterns", patterns)
            exit_codes.append(run_scan(merged))
        raise SystemExit(max(exit_codes))
    raise SystemExit(run_scan(args))


if __name__ == "__main__":
    main()

